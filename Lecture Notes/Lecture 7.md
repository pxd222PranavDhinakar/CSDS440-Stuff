*Overview*
• The *Contingency Table* tracks four kinds of results the *True Positives*, *True Negatives*, *False Positives* and *False Negatives*
- What is accuracy? • What are the weaknesses of accuracy as a metric? (i) (ii) (iii) 
- What is weighted accuracy? Why is it used? • What is precision? Recall? Why are these used? 
- What is specificity? Sensitivity? The F1 score? 
- What is a learning curve? Why is this useful? 
- When the algorithm produces a confidence, we can draw ___ - __ graphs or ___ ___ ___ graphs. 
- To do this we move a t____ down the list of c___ At each point, examples with confidence h____ than the t____ are classified p___, and the rest classified n___. Then we compute the metrics at this threshold. 
• ROC graphs are m______ i___. Also, random classifiers have ROC graphs that are d___. 
• We often use a____ u____ r____ as a comparison statistic. However, ROC graphs can be misleading if the ___
