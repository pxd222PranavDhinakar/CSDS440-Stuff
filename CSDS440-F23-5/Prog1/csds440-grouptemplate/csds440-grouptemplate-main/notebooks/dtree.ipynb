{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0dbb0ee2-5205-41fb-b64b-7d1ee5314851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        This is the class we used to construct the nodes in the DecisionTree\n",
    "        \"\"\"\n",
    "        self.feature_index = feature_index  # feature index on which the node splits\n",
    "        self.threshold = threshold  # threshold value if the feature is continuous\n",
    "        self.left = left  # TreeNode for left subtree\n",
    "        self.right = right  # TreeNode for right subtree\n",
    "        self.value = value  # Value of the leaf node (pure node/class label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f15b451-be0c-47f8-b665-ff2520e77889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import warnings\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('D:\\\\MachineLearning')\n",
    "\n",
    "from typing import Optional, List\n",
    "from numpy import ndarray\n",
    "from sting.classifier import Classifier\n",
    "from sting.data import Feature, FeatureType, parse_c45\n",
    "\n",
    "# In Python, the convention for class names is CamelCase, just like Java! However, the convention for method and\n",
    "# variable names is lowercase_separated_by_underscores, unlike Java.\n",
    "class DecisionTree(Classifier):\n",
    "    \n",
    "    def __init__(self, schema: List[Feature]):\n",
    "        \"\"\"\n",
    "        This is the class where you will implement your decision tree. At the moment, we have provided some dummy code\n",
    "        where this is simply a majority classifier in order to give you an idea of how the interface works. Don't forget\n",
    "        to use all the good programming skills you learned in 132 and utilize numpy optimizations wherever possible.\n",
    "        Good luck!\n",
    "        \"\"\"\n",
    "\n",
    "        self._schema = schema  # For some models (like a decision tree) it makes sense to keep track of the data schema\n",
    "        self._majority_label = 0  # Protected attributes in Python have an underscore prefix\n",
    "        \n",
    "        self.use_information_gain = True\n",
    "        self.root = None\n",
    "        self.max_depth = None\n",
    "        \n",
    "# In Python, instead of getters and setters we have properties: docs.python.org/3/library/functions.html#property\n",
    "    @property\n",
    "    def schema(self):\n",
    "        \"\"\"\n",
    "        Returns: The dataset schema\n",
    "        \"\"\"\n",
    "        return self._schema\n",
    "        \n",
    "    def _entropy(self, y: np.ndarray) :\n",
    "        \"\"\"\n",
    "        Calculate the entropy of the given labels.\n",
    "        A helper method used to calculate the information gain.\n",
    "        \"\"\"\n",
    "        \n",
    "        pos_prob = y.sum() / len(y)\n",
    "        neg_prob = (len(y) - y.sum()) / len(y)\n",
    "    \n",
    "        pos_entropy = - pos_prob * np.log2(pos_prob) if pos_prob > 0 else 0\n",
    "        neg_entropy = - neg_prob * np.log2(neg_prob) if neg_prob > 0 else 0\n",
    "            \n",
    "        entropy = pos_entropy + neg_entropy\n",
    "        return entropy\n",
    "\n",
    "    def _fit_tree(self, X, y, current_depth: int, remaining_features: List[int]) -> TreeNode:\n",
    "        \n",
    "        #Case1 : All samples have the same label(pure nodes)\n",
    "        unique_labels = np.unique(y)\n",
    "        if len(unique_labels) == 1:\n",
    "            return TreeNode(value = unique_labels[0])\n",
    "            \n",
    "        #Case2 : Reached max depth or All attributes are exhausted\n",
    "        if not remaining_features or (self.max_depth is not None and current_depth >= self.max_depth):\n",
    "            majority_label = 1 if np.sum(y) > len(y) / 2 else 0\n",
    "            return TreeNode(value = majority_label)\n",
    "    \n",
    "        best_split_feature, best_threshold = self._determine_split_criterion(X, y, remaining_features)\n",
    "        \n",
    "        #Case3: No good split found\n",
    "        if best_split_feature is None:\n",
    "            majority_label = 1 if np.sum(y) > len(y) / 2 else 0\n",
    "            return TreeNode(value=majority_label)\n",
    "            \n",
    "        #Split the dataset\n",
    "        if self._schema[best_split_feature].ftype == FeatureType.NOMINAL:\n",
    "            # For nominal attributes, simply split using equality\n",
    "            left_indices = X[:, best_split_feature] == best_threshold\n",
    "            right_indices = X[:, best_split_feature] != best_threshold\n",
    "        else:\n",
    "            # For continuous attributes, split using a threshold\n",
    "            left_indices = X[:, best_split_feature] <= best_threshold\n",
    "            right_indices = X[:, best_split_feature] > best_threshold\n",
    "\n",
    "        # Check if one of the splits is empty\n",
    "        if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "            majority_label = 1 if np.sum(y) > len(y) / 2 else 0\n",
    "            return TreeNode(value=majority_label)\n",
    "            \n",
    "       #Remove the attributes that are already tested\n",
    "        next_remaining_features = remaining_features.copy()\n",
    "        next_remaining_features.remove(best_split_feature)\n",
    "    \n",
    "        left_subtree = self._fit_tree(X[left_indices], y[left_indices], current_depth + 1, next_remaining_features)\n",
    "        right_subtree = self._fit_tree(X[right_indices], y[right_indices], current_depth + 1,next_remaining_features)\n",
    "        \n",
    "        return TreeNode(feature_index=best_split_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, weights: Optional[np.ndarray] = None) -> None:\n",
    "        \"\"\"\n",
    "        This is the method where the training algorithm will run.\n",
    "    \n",
    "        Args:\n",
    "            X: The dataset. The shape is (n_examples, n_features).\n",
    "            y: The labels. The shape is (n_examples,)\n",
    "            weights: Weights for each example. Will become relevant later in the course, ignore for now.\n",
    "        \"\"\"\n",
    "    \n",
    "        initial_remaining_features = list(range(X.shape[1]))\n",
    "    \n",
    "        # Begin tree construction starting from the root\n",
    "        self.root = self._fit_tree(X, y, current_depth=0, remaining_features= initial_remaining_features)\n",
    "    \n",
    "        n_zero, n_one = util.count_label_occurrences(y)\n",
    "    \n",
    "        if n_one > n_zero:\n",
    "            self._majority_label = 1\n",
    "        else:\n",
    "            self._majority_label = 0\n",
    "\n",
    "    def _traverse_tree(self, x: ndarray, node: TreeNode):\n",
    "        \"\"\"\n",
    "        This is the method we predict the label of new example coming in our tree\n",
    "    \n",
    "        Args:\n",
    "            x: The specific example in dataset. The shape is (, n_features).\n",
    "            node: The attributes with threshold we used to classify the example. The shape is (n_examples,)\n",
    "            weights: Weights for each example. Will become relevant later in the course, ignore for now.\n",
    "        \"\"\"      \n",
    "        \n",
    "        if node.left is None and node.right is None: #base case: node is a leaf node (meaning that it is labeled)\n",
    "            return node.value\n",
    "            \n",
    "        feature_val = x[node.feature_index]\n",
    "        \n",
    "        if self._schema[node.feature_index].ftype == FeatureType.NOMINAL:\n",
    "            if feature_val == node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            return self._traverse_tree(x, node.right)\n",
    "        else:\n",
    "            if feature_val <= node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            return self._traverse_tree(x, node.right)\n",
    "            \n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This is the method where the decision tree is evaluated.\n",
    "    \n",
    "        Args:\n",
    "            X: The testing data of shape (n_examples, n_features).\n",
    "    \n",
    "        Returns: Predictions of shape (n_examples,), either 0 or 1\n",
    "        \"\"\"\n",
    "        y_pred = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "    def _information_gain(self, X: np.ndarray, y: np.ndarray, feature_index: int) :\n",
    "        \"\"\"\n",
    "        Calculate the information gain after partitioning the dataset by attribute X\n",
    "    \n",
    "        :param feature_index: the index of attribute X used to partition the dataset.\n",
    "        \"\"\"\n",
    "        if self.schema[feature_index].name == 'image_id':\n",
    "            return -float('inf'), None\n",
    "    \n",
    "        entropy_before = self._entropy(y)      \n",
    "        \n",
    "        if(self.schema[feature_index].ftype == FeatureType.NOMINAL):\n",
    "            \n",
    "            conditional_entropy_1 = 0\n",
    "            column_values = X[:,feature_index]\n",
    "            best_split_point = 0\n",
    "            gain_nominal = 0\n",
    "            \n",
    "            for value in np.unique(column_values):\n",
    "                subset_indices = np.where(column_values == value)\n",
    "                subset = y[subset_indices]\n",
    "    \n",
    "                weight = len(subset) / len(y)\n",
    "                entropy = self._entropy(subset)\n",
    "                \n",
    "                conditional_entropy_1 += weight * entropy\n",
    "                gain = entropy_before - conditional_entropy_1\n",
    "                \n",
    "                if gain > gain_nominal:\n",
    "                    best_split_point = value\n",
    "                    gain_nominal = gain\n",
    "                    \n",
    "            return gain_nominal, best_split_point\n",
    "    \n",
    "        elif (self.schema[feature_index].ftype == FeatureType.CONTINUOUS):\n",
    "            \n",
    "            conditional_entropy_2 = 0\n",
    "            best_split_point = 0\n",
    "            gain_continuous = 0\n",
    "            split_points = self._split_continuous_feature(X, y, feature_index)\n",
    "    \n",
    "            for midpoint, left_indices, right_indices in split_points:\n",
    "                \n",
    "                    left_subset = y[left_indices]\n",
    "                    right_subset = y[right_indices]\n",
    "                \n",
    "                    # Skip this midpoint if one of the subsets is empty\n",
    "                    if len(left_subset) == 0 or len(right_subset) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    left_weight = len(left_subset) / len(y)\n",
    "                    right_weight = len(right_subset) / len(y)\n",
    "    \n",
    "                    conditional_entropy_2 = left_weight * self._entropy(left_subset) + right_weight * self._entropy(right_subset)\n",
    "                    gain_midpoint = entropy_before - conditional_entropy_2\n",
    "    \n",
    "                    if gain_midpoint > gain_continuous:\n",
    "                        best_split_point = midpoint\n",
    "                        gain_continuous = gain_midpoint\n",
    "            \n",
    "            return gain_continuous, best_split_point\n",
    "\n",
    "    def _feature_information(self, X:np.array, y:np.array, feature_index: int):\n",
    "        \n",
    "        if self.schema[feature_index].ftype == FeatureType.NOMINAL:           \n",
    "            values, counts = np.unique(X[:, feature_index], return_counts=True)\n",
    "            probs = counts / len(X)\n",
    "            feature_info = -np.sum(probs * np.log2(probs))\n",
    "    \n",
    "            return feature_info\n",
    "            \n",
    "        elif self.schema[feature_index].ftype == FeatureType.CONTINUOUS:\n",
    "            split_points = self._split_continuous_feature(X, y, feature_index)\n",
    "            feature_info_values = []\n",
    "            \n",
    "            for midpoint, left_indices, right_indices in split_points:\n",
    "                \n",
    "                left_subset = X[left_indices]\n",
    "                right_subset = X[right_indices]\n",
    "    \n",
    "                left_probs = len(left_subset) / len(X)\n",
    "                right_probs = len(right_subset) / len(X)\n",
    "    \n",
    "                feature_info = 0\n",
    "                feature_info += - left_probs * np.log2(left_probs) if left_probs > 0 else 0\n",
    "                feature_info += - right_probs * np.log2(right_probs) if right_probs > 0 else 0\n",
    "    \n",
    "                feature_info_values.append(feature_info)\n",
    "            return np.mean(feature_info_values)\n",
    "\n",
    "    def _gain_ratio(self, X:np.array, y:np.array, feature_index: int):\n",
    "    \n",
    "        info_gain = self._information_gain(X, y, feature_index)[0]\n",
    "        feature_info = self._feature_information(X, y, feature_index)\n",
    "        gain_ratio = info_gain / feature_info if feature_info != 0 else 0\n",
    "        \n",
    "        return gain_ratio\n",
    "\n",
    "\n",
    "    def _split_continuous_feature(self, X: np.ndarray, y: np.ndarray, feature_index: int):\n",
    "    \n",
    "        sorted_indices = np.argsort(X[:, feature_index])\n",
    "        X_sorted = X[sorted_indices]\n",
    "        y_sorted = y[sorted_indices]\n",
    "        last_midpoint = None\n",
    "        split_points = []\n",
    "    \n",
    "        for i in range(len(y_sorted) - 1):\n",
    "            if y_sorted[i] != y_sorted[i+1]:\n",
    "                midpoint = ((X_sorted[i,feature_index]) + (X_sorted[i+1,feature_index])) / 2\n",
    "                if midpoint != last_midpoint: \n",
    "                    last_midpoint = midpoint\n",
    "                    left_indices = np.where(X[:, feature_index] <= midpoint)\n",
    "                    right_indices = np.where(X[:, feature_index] > midpoint) \n",
    "                    split_points.append((midpoint, left_indices, right_indices))\n",
    "                \n",
    "        return split_points\n",
    "\n",
    "    # It is standard practice to prepend helper methods with an underscore \"_\" to mark them as protected.\n",
    "    def _determine_split_criterion(self, X: np.ndarray, y: np.ndarray, remaining_features: List[int]):\n",
    "        \"\"\"\n",
    "        Determine decision tree split criterion. This is just an example to encourage you to use helper methods.\n",
    "        Implement this however you like!\n",
    "        \"\"\"\n",
    "        max_criterion_value = -float('inf')\n",
    "        best_feature_index = None\n",
    "        split_point = None\n",
    "    \n",
    "        #Iterate over all features\n",
    "        for feature_index in remaining_features:\n",
    "            if self.use_information_gain:\n",
    "                criterion_value, best_split_point = self._information_gain(X, y, feature_index)\n",
    "            else:\n",
    "                criterion_value, best_split_point = self._gain_ratio(X, y, feature_index)\n",
    "                \n",
    "            if criterion_value > max_criterion_value:\n",
    "                max_criterion_value = criterion_value \n",
    "                best_feature_index = feature_index\n",
    "                split_point = best_split_point\n",
    "        \n",
    "        return best_feature_index, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72c4a814-856e-48d5-9d04-0657ab0297e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print_metrics(dtree: DecisionTree, X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    You will implement this method.\n",
    "    Given a trained decision tree and labelled dataset, Evaluate the tree and print metrics.\n",
    "    \"\"\"\n",
    "    def tree_size(node):\n",
    "        if node == None:\n",
    "            return 0\n",
    "        return 1 + tree_size(node.left) + tree_size(node.right)\n",
    "\n",
    "    def tree_depth(node):\n",
    "        if node == None:\n",
    "            return 0\n",
    "        left_depth = tree_depth(node.left) if node.left is not None else 0\n",
    "        right_depth = tree_depth(node.right) if node.right is not None else 0\n",
    "        \n",
    "        return 1 + max(left_depth, right_depth)\n",
    "\n",
    "    y_hat = dtree.predict(X)\n",
    "    acc = util.accuracy(y, y_hat)\n",
    "    \n",
    "    return acc, tree_size(dtree.root), tree_depth(dtree.root) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bc1225f-109b-4bf3-a518-df74081010af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree(data_path: str, tree_depth_limit: int, use_cross_validation: bool = True, information_gain: bool = True):\n",
    "    \"\"\"\n",
    "    It is highly recommended that you make a function like this to run your program so that you are able to run it\n",
    "    easily from a Jupyter notebook. This function has been PARTIALLY implemented for you, but not completely!\n",
    "\n",
    "    :param data_path: The path to the data.\n",
    "    :param tree_depth_limit: Depth limit of the decision tree\n",
    "    :param use_cross_validation: If True, use cross validation. Otherwise, run on the full dataset.\n",
    "    :param information_gain: If true, use information gain as the split criterion. Otherwise use gain ratio.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # last entry in the data_path is the file base (name of the dataset)\n",
    "    path = os.path.expanduser(data_path).split(os.sep)\n",
    "    file_base = path[-1]  # -1 accesses the last entry of an iterable in Python\n",
    "    root_dir = os.sep.join(path[:-1])\n",
    "    schema, X, y = parse_c45(file_base, root_dir)\n",
    "\n",
    "    if use_cross_validation:\n",
    "        datasets = util.cv_split(X, y, folds=5, stratified=True)\n",
    "    else:\n",
    "        datasets = (X, y, X, y)\n",
    "        \n",
    "    total_accuracy = 0\n",
    "    total_size = 0\n",
    "    total_depth = 0\n",
    "    \n",
    "    for X_train, y_train, X_test, y_test in datasets:\n",
    "        decision_tree = DecisionTree(schema)\n",
    "        decision_tree.use_information_gain = information_gain\n",
    "        decision_tree.max_depth = tree_depth_limit\n",
    "        decision_tree.fit(X_train, y_train)\n",
    "        acc, size, depth = evaluate_and_print_metrics(decision_tree, X_test, y_test)\n",
    "\n",
    "        total_accuracy += acc\n",
    "        total_size += size\n",
    "        total_depth += depth\n",
    "\n",
    "    num_datasets = len(datasets)\n",
    "    avg_accuracy = total_accuracy / num_datasets\n",
    "    avg_size = total_size / num_datasets\n",
    "    avg_depth = total_depth / num_datasets\n",
    "    \n",
    "    print(f'Average Accuracy: {avg_accuracy:.2f}')\n",
    "    print(f'Average Size: {avg_size:.0f}')\n",
    "    print(f'Average Maximum Depth: {avg_depth:.0f}')\n",
    "    print('First Feature:', schema[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0309cfa-f03c-493a-a91d-418337d6f4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    THIS IS YOUR MAIN FUNCTION. You will implement the evaluation of the program here. We have provided argparse code\n",
    "    for you for this assignment, but in the future you may be responsible for doing this yourself.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up argparse arguments\n",
    "    parser = argparse.ArgumentParser(description='Run a decision tree algorithm.')\n",
    "    parser.add_argument('path', metavar='PATH', type=str, help='The path to the data.')\n",
    "    parser.add_argument('depth_limit', metavar='DEPTH', type=int,\n",
    "                        help='Depth limit of the tree. Must be a non-negative integer. A value of 0 sets no limit.')\n",
    "    parser.add_argument('--no-cv', dest='cv', action='store_false',\n",
    "                        help='Disables cross validation and trains on the full dataset.')\n",
    "    parser.add_argument('--use-gain-ratio', dest='gain_ratio', action='store_true',\n",
    "                        help='Use gain ratio as tree split criterion instead of information gain.')\n",
    "    parser.set_defaults(cv=True, gain_ratio=False)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # If the depth limit is negative throw an exception\n",
    "    if args.depth_limit < 0:\n",
    "        raise argparse.ArgumentTypeError('Tree depth limit must be non-negative.')\n",
    "\n",
    "    # You can access args with the dot operator like so:\n",
    "    data_path = os.path.expanduser(args.path)\n",
    "    tree_depth_limit = args.depth_limit\n",
    "    use_cross_validation = args.cv\n",
    "    use_information_gain = not args.gain_ratio\n",
    "\n",
    "    dtree(data_path, tree_depth_limit, use_cross_validation, use_information_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d50016e-a752-4739-bb29-ef6c9be3bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.63\n",
      "Average Size: 3\n",
      "Average Maximum Depth: 1\n",
      "First Feature: Feature(name='geoDistance', ftype=FeatureType.CONTINUOUS)\n"
     ]
    }
   ],
   "source": [
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\spam\",1,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7938974a-5289-44c9-a39e-4dd26a7b3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.72\n",
      "Average Size: 7\n",
      "Average Maximum Depth: 2\n",
      "First Feature: Feature(name='geoDistance', ftype=FeatureType.CONTINUOUS)\n"
     ]
    }
   ],
   "source": [
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\spam\",2,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdb20558-bb05-4b26-b544-1069d292451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.72\n",
      "Average Size: 15\n",
      "Average Maximum Depth: 3\n",
      "First Feature: Feature(name='geoDistance', ftype=FeatureType.CONTINUOUS)\n",
      "Average Accuracy: 0.72\n",
      "Average Size: 31\n",
      "Average Maximum Depth: 4\n",
      "First Feature: Feature(name='geoDistance', ftype=FeatureType.CONTINUOUS)\n",
      "Average Accuracy: 0.73\n",
      "Average Size: 62\n",
      "Average Maximum Depth: 5\n",
      "First Feature: Feature(name='geoDistance', ftype=FeatureType.CONTINUOUS)\n"
     ]
    }
   ],
   "source": [
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\spam\",3,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\spam\",4,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\spam\",5,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f7a5d28e-1596-4f8a-800c-3a01358db9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.67\n",
      "Average Size: 3\n",
      "Average Maximum Depth: 1\n",
      "First Feature: Feature(name='image_id', ftype=FeatureType.NOMINAL, values=[<image_id.1: 1>, <image_id.2: 2>, <image_id.3: 3>, <image_id.4: 4>, <image_id.5: 5>, <image_id.6: 6>, <image_id.7: 7>, <image_id.8: 8>, <image_id.9: 9>, <image_id.10: 10>, <image_id.11: 11>, <image_id.12: 12>, <image_id.13: 13>, <image_id.14: 14>, <image_id.15: 15>, <image_id.16: 16>, <image_id.17: 17>, <image_id.18: 18>, <image_id.19: 19>, <image_id.20: 20>, <image_id.21: 21>, <image_id.22: 22>, <image_id.23: 23>, <image_id.24: 24>, <image_id.25: 25>, <image_id.26: 26>, <image_id.27: 27>, <image_id.28: 28>, <image_id.29: 29>, <image_id.30: 30>, <image_id.31: 31>, <image_id.32: 32>, <image_id.33: 33>, <image_id.34: 34>, <image_id.35: 35>, <image_id.36: 36>, <image_id.37: 37>, <image_id.38: 38>, <image_id.39: 39>, <image_id.40: 40>, <image_id.41: 41>, <image_id.42: 42>, <image_id.43: 43>, <image_id.44: 44>, <image_id.45: 45>, <image_id.46: 46>, <image_id.47: 47>, <image_id.48: 48>, <image_id.50: 49>, <image_id.51: 50>, <image_id.52: 51>, <image_id.53: 52>, <image_id.54: 53>, <image_id.55: 54>, <image_id.56: 55>])\n",
      "Average Accuracy: 0.76\n",
      "Average Size: 7\n",
      "Average Maximum Depth: 2\n",
      "First Feature: Feature(name='image_id', ftype=FeatureType.NOMINAL, values=[<image_id.1: 1>, <image_id.2: 2>, <image_id.3: 3>, <image_id.4: 4>, <image_id.5: 5>, <image_id.6: 6>, <image_id.7: 7>, <image_id.8: 8>, <image_id.9: 9>, <image_id.10: 10>, <image_id.11: 11>, <image_id.12: 12>, <image_id.13: 13>, <image_id.14: 14>, <image_id.15: 15>, <image_id.16: 16>, <image_id.17: 17>, <image_id.18: 18>, <image_id.19: 19>, <image_id.20: 20>, <image_id.21: 21>, <image_id.22: 22>, <image_id.23: 23>, <image_id.24: 24>, <image_id.25: 25>, <image_id.26: 26>, <image_id.27: 27>, <image_id.28: 28>, <image_id.29: 29>, <image_id.30: 30>, <image_id.31: 31>, <image_id.32: 32>, <image_id.33: 33>, <image_id.34: 34>, <image_id.35: 35>, <image_id.36: 36>, <image_id.37: 37>, <image_id.38: 38>, <image_id.39: 39>, <image_id.40: 40>, <image_id.41: 41>, <image_id.42: 42>, <image_id.43: 43>, <image_id.44: 44>, <image_id.45: 45>, <image_id.46: 46>, <image_id.47: 47>, <image_id.48: 48>, <image_id.50: 49>, <image_id.51: 50>, <image_id.52: 51>, <image_id.53: 52>, <image_id.54: 53>, <image_id.55: 54>, <image_id.56: 55>])\n",
      "Average Accuracy: 0.77\n",
      "Average Size: 15\n",
      "Average Maximum Depth: 3\n",
      "First Feature: Feature(name='image_id', ftype=FeatureType.NOMINAL, values=[<image_id.1: 1>, <image_id.2: 2>, <image_id.3: 3>, <image_id.4: 4>, <image_id.5: 5>, <image_id.6: 6>, <image_id.7: 7>, <image_id.8: 8>, <image_id.9: 9>, <image_id.10: 10>, <image_id.11: 11>, <image_id.12: 12>, <image_id.13: 13>, <image_id.14: 14>, <image_id.15: 15>, <image_id.16: 16>, <image_id.17: 17>, <image_id.18: 18>, <image_id.19: 19>, <image_id.20: 20>, <image_id.21: 21>, <image_id.22: 22>, <image_id.23: 23>, <image_id.24: 24>, <image_id.25: 25>, <image_id.26: 26>, <image_id.27: 27>, <image_id.28: 28>, <image_id.29: 29>, <image_id.30: 30>, <image_id.31: 31>, <image_id.32: 32>, <image_id.33: 33>, <image_id.34: 34>, <image_id.35: 35>, <image_id.36: 36>, <image_id.37: 37>, <image_id.38: 38>, <image_id.39: 39>, <image_id.40: 40>, <image_id.41: 41>, <image_id.42: 42>, <image_id.43: 43>, <image_id.44: 44>, <image_id.45: 45>, <image_id.46: 46>, <image_id.47: 47>, <image_id.48: 48>, <image_id.50: 49>, <image_id.51: 50>, <image_id.52: 51>, <image_id.53: 52>, <image_id.54: 53>, <image_id.55: 54>, <image_id.56: 55>])\n",
      "Average Accuracy: 0.77\n",
      "Average Size: 28\n",
      "Average Maximum Depth: 4\n",
      "First Feature: Feature(name='image_id', ftype=FeatureType.NOMINAL, values=[<image_id.1: 1>, <image_id.2: 2>, <image_id.3: 3>, <image_id.4: 4>, <image_id.5: 5>, <image_id.6: 6>, <image_id.7: 7>, <image_id.8: 8>, <image_id.9: 9>, <image_id.10: 10>, <image_id.11: 11>, <image_id.12: 12>, <image_id.13: 13>, <image_id.14: 14>, <image_id.15: 15>, <image_id.16: 16>, <image_id.17: 17>, <image_id.18: 18>, <image_id.19: 19>, <image_id.20: 20>, <image_id.21: 21>, <image_id.22: 22>, <image_id.23: 23>, <image_id.24: 24>, <image_id.25: 25>, <image_id.26: 26>, <image_id.27: 27>, <image_id.28: 28>, <image_id.29: 29>, <image_id.30: 30>, <image_id.31: 31>, <image_id.32: 32>, <image_id.33: 33>, <image_id.34: 34>, <image_id.35: 35>, <image_id.36: 36>, <image_id.37: 37>, <image_id.38: 38>, <image_id.39: 39>, <image_id.40: 40>, <image_id.41: 41>, <image_id.42: 42>, <image_id.43: 43>, <image_id.44: 44>, <image_id.45: 45>, <image_id.46: 46>, <image_id.47: 47>, <image_id.48: 48>, <image_id.50: 49>, <image_id.51: 50>, <image_id.52: 51>, <image_id.53: 52>, <image_id.54: 53>, <image_id.55: 54>, <image_id.56: 55>])\n",
      "Average Accuracy: 0.80\n",
      "Average Size: 47\n",
      "Average Maximum Depth: 5\n",
      "First Feature: Feature(name='image_id', ftype=FeatureType.NOMINAL, values=[<image_id.1: 1>, <image_id.2: 2>, <image_id.3: 3>, <image_id.4: 4>, <image_id.5: 5>, <image_id.6: 6>, <image_id.7: 7>, <image_id.8: 8>, <image_id.9: 9>, <image_id.10: 10>, <image_id.11: 11>, <image_id.12: 12>, <image_id.13: 13>, <image_id.14: 14>, <image_id.15: 15>, <image_id.16: 16>, <image_id.17: 17>, <image_id.18: 18>, <image_id.19: 19>, <image_id.20: 20>, <image_id.21: 21>, <image_id.22: 22>, <image_id.23: 23>, <image_id.24: 24>, <image_id.25: 25>, <image_id.26: 26>, <image_id.27: 27>, <image_id.28: 28>, <image_id.29: 29>, <image_id.30: 30>, <image_id.31: 31>, <image_id.32: 32>, <image_id.33: 33>, <image_id.34: 34>, <image_id.35: 35>, <image_id.36: 36>, <image_id.37: 37>, <image_id.38: 38>, <image_id.39: 39>, <image_id.40: 40>, <image_id.41: 41>, <image_id.42: 42>, <image_id.43: 43>, <image_id.44: 44>, <image_id.45: 45>, <image_id.46: 46>, <image_id.47: 47>, <image_id.48: 48>, <image_id.50: 49>, <image_id.51: 50>, <image_id.52: 51>, <image_id.53: 52>, <image_id.54: 53>, <image_id.55: 54>, <image_id.56: 55>])\n"
     ]
    }
   ],
   "source": [
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\volcanoes\",1,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\volcanoes\",2,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\volcanoes\",3,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\volcanoes\",4,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\volcanoes\",5,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "348fd9ef-f7cf-4ec8-afa6-ac0890008647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.98\n",
      "Average Size: 3\n",
      "Average Maximum Depth: 1\n",
      "First Feature: Feature(name='Repealing-the-Job-Killing-Health-Care-Law-Act', ftype=FeatureType.NOMINAL, values=[<Repealing-the-Job-Killing-Health-Care-Law-Act.-: 1>, <Repealing-the-Job-Killing-Health-Care-Law-Act.0: 2>, <Repealing-the-Job-Killing-Health-Care-Law-Act.+: 3>])\n",
      "Average Accuracy: 0.97\n",
      "Average Size: 5\n",
      "Average Maximum Depth: 2\n",
      "First Feature: Feature(name='Repealing-the-Job-Killing-Health-Care-Law-Act', ftype=FeatureType.NOMINAL, values=[<Repealing-the-Job-Killing-Health-Care-Law-Act.-: 1>, <Repealing-the-Job-Killing-Health-Care-Law-Act.0: 2>, <Repealing-the-Job-Killing-Health-Care-Law-Act.+: 3>])\n",
      "Average Accuracy: 0.97\n",
      "Average Size: 7\n",
      "Average Maximum Depth: 3\n",
      "First Feature: Feature(name='Repealing-the-Job-Killing-Health-Care-Law-Act', ftype=FeatureType.NOMINAL, values=[<Repealing-the-Job-Killing-Health-Care-Law-Act.-: 1>, <Repealing-the-Job-Killing-Health-Care-Law-Act.0: 2>, <Repealing-the-Job-Killing-Health-Care-Law-Act.+: 3>])\n",
      "Average Accuracy: 0.97\n",
      "Average Size: 9\n",
      "Average Maximum Depth: 4\n",
      "First Feature: Feature(name='Repealing-the-Job-Killing-Health-Care-Law-Act', ftype=FeatureType.NOMINAL, values=[<Repealing-the-Job-Killing-Health-Care-Law-Act.-: 1>, <Repealing-the-Job-Killing-Health-Care-Law-Act.0: 2>, <Repealing-the-Job-Killing-Health-Care-Law-Act.+: 3>])\n",
      "Average Accuracy: 0.97\n",
      "Average Size: 11\n",
      "Average Maximum Depth: 5\n",
      "First Feature: Feature(name='Repealing-the-Job-Killing-Health-Care-Law-Act', ftype=FeatureType.NOMINAL, values=[<Repealing-the-Job-Killing-Health-Care-Law-Act.-: 1>, <Repealing-the-Job-Killing-Health-Care-Law-Act.0: 2>, <Repealing-the-Job-Killing-Health-Care-Law-Act.+: 3>])\n"
     ]
    }
   ],
   "source": [
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\voting\",1,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\voting\",2,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\voting\",3,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\voting\",4,True,True)\n",
    "dtree(\"D:\\\\MachineLearning\\\\440data\\\\voting\",5,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d681617-2b4a-4ce6-b447-0ccdfd0b4829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91f4ab-b4fb-4fdc-a74d-443fbc67f632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1231f94-0b29-43b2-84c7-eb21bcbd8001",
   "metadata": {},
   "source": [
    "**(a)** For question (a), I've made every effort to produce the desired output. Nonetheless, despite my attempts to exclude the attribute \"image_id\" when determining information gain, I couldn't achieve the expected outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba93a8-e657-4c52-8c9b-532fe86d8489",
   "metadata": {},
   "source": [
    "**(b)** I was unsuccessful in determining the initial test. The algorithm possibly defaults to a nominal attribute with multiple values for its primary assessment. This is a recognized limitation of the ID3 algorithm when using information gain; it inherently favors attributes with multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910a0b9-0a1e-4a81-b1ba-2f7e265d6b2a",
   "metadata": {},
   "source": [
    "**(c)** As I was unable to exclude the \"image_id\" attribute from the volcanoes dataset during the testing phase, I couldn't generate a relevant graph to provide a visual explanation. Although I've poured considerable effort into constructing the algorithm, its performance on the Spam dataset is suboptimal, especially when the depth is set to 3. Based on the results from varying the depth between 1 to 5 on the Spam dataset, I can hypothesize that accuracy sees a significant rise in the initial depths. However, after a certain point, it plateaus, and any subsequent increase in depth only results in marginal fluctuations in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa2c45-11b1-49f8-ad3d-03a00b25d516",
   "metadata": {},
   "source": [
    "# Research Extension: Explore the Interaction Between Gini Impurity and Information Gain in Decision Trees\r\n",
    "\r\n",
    "## 1. Introduction\r\n",
    "\r\n",
    "Decision trees depends on different split criteria to determine how data should be partitioned to create child nodes. Among these criteria, Gini Impurity and Information Gain are commonly used. We try to understand their performance differences and the scenarios in which one may outperform the other. We may be able to combine these intersections and introduces a novel hybrid metric, tentatively termed \"Gini-Information Gain\".\r\n",
    "\r\n",
    "## 2. Background\r\n",
    "\r\n",
    "**Information Gain (based on Entropy)**\r\n",
    "\r\n",
    "1. Information Gain measures the amount or increase in information (or purity) obtained by partitioning a dataset into subsets.\r\n",
    "2. It computes using entropy, a measure of impurity in the dataset.\r\n",
    "\r\n",
    "Depending on the nature of the IG, the pros and cons are clear:\r\n",
    "\r\n",
    "**Advantages**:\r\n",
    "- Recognizes and prioritizes important attributes, leading to more discriminative decisions at the top layers of the decision tree.\r\n",
    "\r\n",
    "**Drawbacks**:\r\n",
    "- Might be biased towards attributes with many values even if those attributes do not contribute significantly to classification.\r\n",
    "\r\n",
    "**Gini Impurity**:\r\n",
    "\r\n",
    "1. Gini Impurity measures the probability of a randomly selected sample being incorrectly classified.\r\n",
    "2. It tends to be computationally more efficient as it doesn't involve the calculation of logarithmic functions.\r\n",
    "\r\n",
    "**Advantages**:\r\n",
    "- Computationally simpler and performs well for splits on continuous attributes without being biased towards attributes with more values.\r\n",
    "\r\n",
    "**Drawbacks**:\r\n",
    "- Might not always yield the most optimal tree structure as it may overlook some crucial attributes.\r\n",
    "\r\n",
    "## 3. Hypothesis\r\n",
    "\r\n",
    "As the pros and cons we discussed above for each method, combining the selectiveness of information gain and the simplicity of Gini impurity might produce a superior split criterion that's adaptable across various datasets.\r\n",
    "\r\n",
    "## 4. Experimental Design\r\n",
    "\r\n",
    "**Dataset Selection**: a dataset with a mix of continuous and categorical attributes, especially those where some attributes have many potential values, to test the adaptability of the new criterion to these scenarios.\r\n",
    "\r\n",
    "**New Metric - Combining Information Gain with Gini Impurity**: \r\n",
    "\r\n",
    "Develop a formula that takes both into account when computing split points. For instance, one could combine the weighted information gain with Gini impurity, adjusting the weights based on the type of attribute or other factors. Intuitively thinking, we definitely would like to weight less for those attributes having multiple values. \r\n",
    "\r\n",
    "**Procedure**:\r\n",
    "\r\n",
    "1. Construct three decision trees for each dataset.\r\n",
    "2. Evaluate using cross-validation.\r\n",
    "3. Analyze computational efficiency, tree depth, and other relevant metrics like F1 score etc.\r\n",
    "\r\n",
    "## 5. The Answers We Expected\r\n",
    "\r\n",
    "1. Does the new criterion perform better for attributes with multiple potential values, and somehow we also prioritize those attributes with ablity to provide more information?\r\n",
    "2. In terms of computational efficiency, is the new criterion comparable to pure Gini impurity? Hopefully, it falls between the using pure IG and pure Gini.\r\n",
    "3. Is this new criteria better to predict the novel samples in terms of the eavluating metrics?\r\n",
    "4. We may also need to test on different datasets with distinct types, like balanced, unbalanced, individual feature with multi-values, etc. And we should try to explore what kind of dataset the new criteria is better to deal with. \r\n",
    "ics.tors.n.tions.\n",
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
